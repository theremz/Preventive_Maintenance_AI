{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import pyodbc\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL data source extractions from database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Unit installation and Removal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I don't want to share the information of the SQL database, I will simply explain how the query was made to get the time between two unit installation.\n",
    "\n",
    "The database where the information of the units is located is a transactional database. In other words, each time an action is created, a new data entry is made. In our case, we wanted to get the moment a unit was installed and remove. To acquire this information I have to do a self join and get the minimum value between two installations. \n",
    "\n",
    "First problematic: The self join query cannot be done on the unit serial number since there is no restraint on the user data entry. For example, the same unit could have the following serial number FV-0001 or FV0001 or 0001.\n",
    "\n",
    "Since it is not possible to self join the table on the serial number, I have to self join the table on main_unit number and unit position. \n",
    "\n",
    "This will help us get the lifespan, moment of installation, moments of removal and unit functionning time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting The Active_lifespan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second query will help get the real running moment of the unit. Since the unit is installed and doesn't run all the time, it will be interesting to get a clear view on how long each unit ran before the being removed. The query is a simple select."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the incident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During a unit lifespan, it is possible that an event occur which can be clear indicator of the malfunction of a unit. Luckily, there is an existing SQL database available to extract this information. The query is a simple select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python query example with a simple SQL select\n",
    "\n",
    "mySql_query= \"\"\"Select * from Table\n",
    "     \"\"\"\n",
    "connection = pyodbc.connect(driver='{SQL Server}', \n",
    "                                server='Serveur1', \n",
    "                                database='Database',               \n",
    "                                trusted_connection='yes')\n",
    "Data_Unit=pd.read_sql(mySql_query, connection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF SQL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNAPSHOT FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the code next steps:\n",
    "\n",
    "Right now, the data shows when the unit was installed and when the unit was removed. Since we are trying to predict when to remove the units, we need to modify the current dataset to represent reality. The course of action decided will be to create a time series by partitioning the data. \n",
    "\n",
    "For example, if a unit was installed from the 2019-01-01 to the 2020-01-01, it means it was installed 365 days. We can partition the data into four (each 100 days) to get the desired snapshots. Those snapshot will be used to represent the moment we run the model when it is in production.\n",
    "\n",
    "The next steps will be to add features between the unit installation date and the snapshot date. This step will create the cumulative time series that we will be using to train the model. \n",
    "\n",
    "In other words, the model will be trained based on time series that represent a cumulation of events of features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This funciton is responsible for created the snapshot on the data. You can see that data is partitionned \n",
    "### every 20 days.\n",
    "### Since it is still unclear if the business wants to predict the unit failure 60-90-120 days before failure\n",
    "\n",
    "def TimeLine2(data):\n",
    "    \n",
    "   #BUG possible a revoir \n",
    "    Columns=['Main_Unit_Number',\n",
    "'Serial_Number_Installed',\n",
    "'Unit_Position',\n",
    "'Serial_Number_Removed',\n",
    "'Installation_date',\n",
    "'Remove_date',\n",
    "'Main_Unit_Type',\n",
    "'Unit_Active_Time',\n",
    "'Unit_Lifespan',\n",
    "'Daily_Active_Time']\n",
    "    position=0    \n",
    "    Unit_life=pd.DataFrame(columns=Columns)\n",
    "    Snapshot=[]\n",
    "    \n",
    "    for row in range(data.shape[0]):\n",
    "        \n",
    "        #For each rows, installation Date and removing Date are initialized\n",
    "        insta=data.at[row,'Installation_date']\n",
    "        remove=data.at[row,'Remove_date']\n",
    "\n",
    "        while insta<remove:\n",
    "            \n",
    "            insta=insta+ datetime.timedelta(days=20)\n",
    "            \n",
    "            Unit_life=Unit_life.append(data.iloc[row:row+1])\n",
    "\n",
    "            #Snapshot=Snapshot.append(insta)\n",
    "            if insta<remove:\n",
    "                Snapshot.append(insta)\n",
    "            else:\n",
    "                Snapshot.append(data.iloc[row,8])\n",
    "\n",
    "    Unit_life['Snapshot_Date']=Snapshot\n",
    "    return(Unit_life)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataframe creations functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The count weather function calculate all the weather event that the unit experienced between the installation\n",
    "## and the Snapshot moment.\n",
    "\n",
    "\n",
    "def count_weather(data,Temperatures):\n",
    "    \n",
    "\n",
    "    Columns=[\n",
    "    'date',\n",
    "    '-15 and under',\n",
    "    '-10 to -14',\n",
    "    '-5 to -9',\n",
    "    '-0 to -4',\n",
    "    '15 and over',\n",
    "    '10 to 14',\n",
    "    '5 to 9',\n",
    "    '1 to 4',\n",
    "    'Snow',\n",
    "    'Rain',\n",
    "    'Ice Storm'\n",
    "             ]\n",
    "    newcolumns=pd.DataFrame(columns=Columns)\n",
    "    \n",
    "    data_with_weather=data.join(newcolumns)\n",
    "    \n",
    "    #Weather Date is removed\n",
    "    data_with_weather=data_with_weather.drop(['date'], axis=1)\n",
    "    Columns.remove('date')\n",
    "    \n",
    "    for row in range(data.shape[0]):\n",
    "        \n",
    "        #For each unit(rows), installation Date and Snapshot_Date are initialized\n",
    "        insta=data.at[row, 'Installation_date'] #[row]['Installation_date']\n",
    "        snap=data.at[row, 'Snapshot_Date']\n",
    "        \n",
    "        for col in Columns:\n",
    "\n",
    "            #Days for each range of temperature are summed between considered Dates for each unit\n",
    "            data_with_weather.at[row,col] = Temperatures[col][(Temperatures['date'] > insta) & (Temperatures['date'] <= snap)].sum()\n",
    "    \n",
    "    return(data_with_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main Unit Classification function\n",
    "### Based on data profiling (see other document) and engineering review, the main unit can be divided\n",
    "### into 2 categories.\n",
    "\n",
    "def Type_Classification(x):\n",
    "    if x=='H':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since the position of the unit is not clean, it is really important to standardized it numerically. \n",
    "## Instead of having 1R, 01R \n",
    "## or 01-R. The posiontions will be standardized 1,2,3 nad 4.\n",
    "\n",
    "def PositionFunc(position):\n",
    "    for x in position:\n",
    "        if x.isdigit():\n",
    "            if int(x)>0:\n",
    "                value=x\n",
    "                return value\n",
    "    return 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Unit['Installation_date']= pd.to_datetime(Data_Unit['Installation_date']) \n",
    "Data_Unit['Remove_date']= pd.to_datetime(Data_Unit['Remove_date'])\n",
    "  \n",
    "Malfunction_data['Mal_Date']=pd.to_datetime(Malfunction_data['Mal_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the position entry\n",
    "\n",
    "Data_Unit['Key_Position']=Data_Unit['EquipmentNumber'].astype('str')+Data_Unit['CleanPosition'].astype(str)\n",
    "\n",
    "#Add Column\n",
    "Data_Unit['Installation_month']=Data_Unit['Installation_date'].apply(lambda x:x.month)\n",
    "Data_Unit['Remove_month']=Data_Unit['Remove_date'].apply(lambda x:x.month)\n",
    "Data_Unit['EquipmentTypeN']=Data_Unit['EquipmentType'].apply(lambda x:EquipmentType(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Column\n",
    "Data_Unit['Installation_month']=Data_Unit['Installation_date'].apply(lambda x:x.month)\n",
    "Data_Unit['Remove_month']=Data_Unit['Remove_date'].apply(lambda x:x.month)\n",
    "Data_Unit['EquipmentTypeN']=Data_Unit['EquipmentType'].apply(lambda x:EquipmentType(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Unit['CleanPosition']=Data_Unit['MainComponentPositionCode'].apply(PositionFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Unit_Life=TimeLine2(Data_Unit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Unit_Life['Snapshot_Date']=pd.to_datetime(Data_Unit_Life['Snapshot_Date']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA PREPARATION ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#clean the position entry\n",
    "\n",
    "#Should be done before\n",
    "\n",
    "\n",
    "#create a position key to merge malfunction data with unit change data\n",
    "Malfunction_data['Key_Position']=Malfunction_data['Main_Unit_Number'].astype(str)+Malfunction_data['position'].astype(str)\n",
    "\n",
    "\n",
    "#create a super key to merge unit change with malfunction data matrix\n",
    "Data_Unit_Life['super_key']=(Data_Unit_Life['Installation_date'].astype(str)+\n",
    "                             Data_Unit_Life['Snapshot_Date'].astype(str)+\n",
    "                             Data_Unit_Life['Key_Position'].astype(str)+\n",
    "                             Data_Unit_Life['CleanPosition'].astype(str))\n",
    "\n",
    "\n",
    "Malfunction_data=Data_Unit_Life.merge(right=Malfunction_data,on='Key_Position',how='left')\n",
    "\n",
    "Malfunction_data['super_key']= (Malfunction_data['Installation_date'].astype(str)+\n",
    "                          Malfunction_data['Snapshot_Date'].astype(str)+\n",
    "                          Malfunction_data['Key_Position'].astype(str)+\n",
    "                          Malfunction_data['position'].astype(str))\n",
    "\n",
    "#Create sub matrix with level condition and date level condition\n",
    "\n",
    "Mal_list_LV=[0,1,2,3]\n",
    "\n",
    "for i in Mal_list:\n",
    "    \n",
    "    LV=Malfunction_data[(Malfunction_data['Installation_date']<Malfunction_data['Function_Time'])&(\n",
    "        Malfunction_data['Snapshot_Date']>=Malfunction_data['Function_Time'])&(Malfunction_data['Level']==i)]\n",
    "\n",
    "    LV=LV.groupby('super_key'\n",
    "    ).count()\n",
    "    LV=LV.reset_index()\n",
    "    LV['L0']=LV[['Main_Unit_Number']]\n",
    "    LV=LV[['super_key','L0']]\n",
    "\n",
    "    Data_Unit_Life=Data_Unit_Life.merge(right=LV,on='super_key',how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datat= pd.read_csv(r'WheatherData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datat['date']=pd.to_datetime(datat['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index to enable date match\n",
    "Data_Unit_Life=Data_Unit_Life.reset_index(drop=True)\n",
    "\n",
    "#count wetaher date\n",
    "Final_data=count_weather(Data_Unit_Life,datat)\n",
    "\n",
    "\n",
    "#shape check fo duplicate\n",
    "print(Final_data.shape)\n",
    "\n",
    "\n",
    "Final_data['RUL']=[int(d.days) for d in (Final_data['Remove_date']-Final_data['Snapshot_Date'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_data.to_csv('Final_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
